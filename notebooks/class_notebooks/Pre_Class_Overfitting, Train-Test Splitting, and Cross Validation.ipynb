{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v0hVGSvAnLva"
   },
   "source": [
    "# Overfitting, Test-Train Splitting, and Cross-Validation\n",
    "This notebook will discuss overfitting data and cross-validation using the [``numpy``](http://www.numpy.org/), [``matplotlib``](https://matplotlib.org/), and [``scikit-learn``](https://scikit-learn.org/stable/) libraries.\n",
    "\n",
    "``numpy`` is a scientific computing library for Python. It is \"lower level\" than other libraries that we'll be using later in this course (in this context \"lower level\" means \"has more nitty-gritty details\"). Most of the data science libraries for Python are built on top of ``numpy``.\n",
    "\n",
    "``matplotlib`` is a visualization library for Python. Its name comes from the fact that was originally based on the plotting functionality of Matlab. Most of the visualization libraries that one will come across in Python are built upon ``matplotlib``.\n",
    "\n",
    "``scikit-learn`` is a library that has many machine learning algorithms implemented. We will be using ``scikit-learn`` more in the next few examples, but for now we're just using it to split our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-01T23:00:24.868591Z",
     "iopub.status.busy": "2023-03-01T23:00:24.867286Z",
     "iopub.status.idle": "2023-03-01T23:00:25.530579Z",
     "shell.execute_reply": "2023-03-01T23:00:25.529343Z",
     "shell.execute_reply.started": "2023-03-01T23:00:24.868414Z"
    },
    "executionInfo": {
     "elapsed": 983,
     "status": "ok",
     "timestamp": 1675261173013,
     "user": {
      "displayName": "Donatella Taurasi",
      "userId": "00673418551725000492"
     },
     "user_tz": -60
    },
    "id": "STuc-MA9oyMd",
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/0q/3y5dk4390kqc86qh28v817lw0000gn/T/ipykernel_11001/4189074138.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# We'll be just using a couple of functions from sklearn's model_selection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# module, so don't bother with an abbreviation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "# Most Python code uses the abbreviation np for numpy\n",
    "import numpy as np\n",
    "# Most python code uses the abbreviation plt for matplotlib's pyplot module\n",
    "import matplotlib.pyplot as plt\n",
    "# We'll be just using a couple of functions from sklearn's model_selection\n",
    "# module, so don't bother with an abbreviation.\n",
    "import sklearn.model_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rXchp_YHghEf"
   },
   "source": [
    "# Our goal\n",
    "\n",
    "Our goal will be to find the best model to fit our data without over-fitting.\n",
    "\n",
    "The class of models that we'll be considering is all polynomials, so, a linear fit (1st degree polynomial), a quadratic fit (2nd degree polynomial), a cubic fit (3rd degree polynomial), etc.\n",
    "\n",
    "To rephrase the above, our goal is to find the number $P$ such that a $P$th degree polynomial is the best model for fitting our data (we have been referring to this number as $\\lambda$).\n",
    "\n",
    "The parameters for any given polynomial fit are the coefficients of that polynomial (so, for a line $y=\\beta x+\\alpha$, the parameters are $\\beta$ and $\\alpha$). Sometimes this parameter for the model $P$ is called a \"hyperparameter\", to distinguish it from the parameters of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "midIpoPOpPee"
   },
   "source": [
    "# Generating our Data\n",
    "In this example, we'll generate some random data that we'll be using to work on. First, let's create a variable ``n`` for the number of data points we'll have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-01T23:00:36.192654Z",
     "iopub.status.busy": "2023-03-01T23:00:36.192345Z",
     "iopub.status.idle": "2023-03-01T23:00:36.196608Z",
     "shell.execute_reply": "2023-03-01T23:00:36.195851Z",
     "shell.execute_reply.started": "2023-03-01T23:00:36.192624Z"
    },
    "executionInfo": {
     "elapsed": 374,
     "status": "ok",
     "timestamp": 1675261419454,
     "user": {
      "displayName": "Donatella Taurasi",
      "userId": "00673418551725000492"
     },
     "user_tz": -60
    },
    "id": "gfl3dp2ZpdsK"
   },
   "outputs": [],
   "source": [
    "n = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9O2mCZW4pfOf"
   },
   "source": [
    "While we want our data to be random, for our purposes in this example we will want the data that we get to be repeatable (otherwise we could never have slides that use the same data as our notebook!). To do this, we set something called a random seed using [``np.random.seed``](https://docs.scipy.org/doc/numpy-1.10.0/reference/generated/numpy.random.seed.html). This ensures that when we generate random data using ``np.random``, we will always get the same thing. Here we use 7 as an arbitrary choice (not really, it's my favorite number)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-01T23:00:36.612526Z",
     "iopub.status.busy": "2023-03-01T23:00:36.612206Z",
     "iopub.status.idle": "2023-03-01T23:00:36.616828Z",
     "shell.execute_reply": "2023-03-01T23:00:36.615950Z",
     "shell.execute_reply.started": "2023-03-01T23:00:36.612495Z"
    },
    "executionInfo": {
     "elapsed": 242,
     "status": "ok",
     "timestamp": 1675261572106,
     "user": {
      "displayName": "Donatella Taurasi",
      "userId": "00673418551725000492"
     },
     "user_tz": -60
    },
    "id": "FeL1E8BWqEj_"
   },
   "outputs": [],
   "source": [
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0FzXleYDpMiD"
   },
   "source": [
    "If we want to start getting truly (or, less predictable) random numbers again, then we would just call ``np.random.seed()`` with no parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w7qtCCpwqSIm"
   },
   "source": [
    "## The x values\n",
    "\n",
    "Let's define ``x`` to be a set of points selected uniformly at random from the interval $[0, 3.14)$, using the [``np.random.uniform``](https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.uniform.html) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-03-01T23:00:37.412361Z",
     "iopub.status.busy": "2023-03-01T23:00:37.411962Z",
     "iopub.status.idle": "2023-03-01T23:00:37.449333Z",
     "shell.execute_reply": "2023-03-01T23:00:37.448274Z",
     "shell.execute_reply.started": "2023-03-01T23:00:37.412323Z"
    },
    "executionInfo": {
     "elapsed": 225,
     "status": "ok",
     "timestamp": 1675261573689,
     "user": {
      "displayName": "Donatella Taurasi",
      "userId": "00673418551725000492"
     },
     "user_tz": -60
    },
    "id": "l1-3v7wPqTZL",
    "outputId": "c220f470-3ba2-4024-a801-b473ed4224b4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.23960803, 2.44894501, 1.37660499, 2.27168066, 3.07088707,\n",
       "       1.69087703, 1.57351826, 0.22624056, 0.8428984 , 1.56963105,\n",
       "       2.13278219, 2.52374057, 1.19615516, 0.20704013, 0.90477718,\n",
       "       2.85612368, 0.67003001, 1.41966924, 2.9239869 , 0.07818357,\n",
       "       1.8857236 , 2.98340663, 0.72315104, 1.72225835, 2.8546631 ,\n",
       "       0.41815206, 1.6435155 , 2.35628696, 2.10070158, 1.46874398])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.random.uniform(0,3.14,n)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lKHgiRIeKVSG"
   },
   "source": [
    "Note that the first number in the series should be 0.23960803."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xeW7hUTzqlko"
   },
   "source": [
    "## The y values\n",
    "Let's define ``y`` using the function $y=\\sin(x)$ with some normally distributed noise added. The normally distributed noise is generated using the [``np.random.normal``](https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.normal.html) function. We'll have the noise's distribution be centered at $0$ and have a standard deviation of $0.1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 213,
     "status": "ok",
     "timestamp": 1675261670945,
     "user": {
      "displayName": "Donatella Taurasi",
      "userId": "00673418551725000492"
     },
     "user_tz": -60
    },
    "id": "C4QE-7rHqh7K",
    "outputId": "f1ec3cb1-c53b-4788-d159-93923e35e336"
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "y = np.sin(x)+np.random.normal(0, 0.1, n)\n",
    "#y = np.sin(x)\n",
    "\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b47u2T1iLuKm"
   },
   "source": [
    "Note that the first number should be 0.39975641."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ror9-F5yrZq3"
   },
   "source": [
    "## Plotting the data\n",
    "Let's create a scatterplot of the data using the [``plt.scatter``](https://matplotlib.org/api/_as_gen/matplotlib.pyplot.scatter.html) function.\n",
    "\n",
    "One thing to note about ``matplotlib`` is that each call to ``plt`` will draw additional things on to your \"current\" plot. Only once you have drawn everything you need should you call [``plt.show``](https://matplotlib.org/api/_as_gen/matplotlib.pyplot.show.html), which will make the plot appear on-screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "executionInfo": {
     "elapsed": 254,
     "status": "ok",
     "timestamp": 1675261673841,
     "user": {
      "displayName": "Donatella Taurasi",
      "userId": "00673418551725000492"
     },
     "user_tz": -60
    },
    "id": "9MsrAUjDr3G-",
    "outputId": "f753beab-a683-4cd0-a1e6-cd5060bbb45a"
   },
   "outputs": [],
   "source": [
    "plt.scatter(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7rSr0WgLhVGb"
   },
   "source": [
    "# Fitting Various Models to Our Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zEr9VeEm91eL"
   },
   "source": [
    "## Linear regression\n",
    "\n",
    "In this section we will fit the data to a line. In future examples we'll use more sophisticated regressions packages from ``statsmodels``, but for now we will just use the [``np.polyfit``](https://docs.scipy.org/doc/numpy/reference/generated/numpy.polyfit.html) function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vhQO7LXBEdE6"
   },
   "source": [
    "### Computing the linear fit\n",
    "\n",
    "When we call ``np.polyfit`` with our data, it will return a ``np.array`` with the polynomial coefficients, which we convert to a [``np.poly1d``](https://docs.scipy.org/doc/numpy/reference/generated/numpy.poly1d.html), which is ``numpy``'s polynomial class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 252,
     "status": "ok",
     "timestamp": 1675261781537,
     "user": {
      "displayName": "Donatella Taurasi",
      "userId": "00673418551725000492"
     },
     "user_tz": -60
    },
    "id": "v4zfJSR8TpgU",
    "outputId": "ee21ac7b-b9ec-4c2f-a8c0-d9211efaf7e8"
   },
   "outputs": [],
   "source": [
    "linear_fit_coefficients = np.polyfit(x, y, deg=1)\n",
    "print(linear_fit_coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 217,
     "status": "ok",
     "timestamp": 1675261843560,
     "user": {
      "displayName": "Donatella Taurasi",
      "userId": "00673418551725000492"
     },
     "user_tz": -60
    },
    "id": "pXnBGvabsEy5",
    "outputId": "7b75b5bf-5ce1-4211-ccbe-c79d50c02623"
   },
   "outputs": [],
   "source": [
    "linear_fit = np.poly1d(linear_fit_coefficients)\n",
    "print('y = ')\n",
    "print(linear_fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rWMkdYHbEjWe"
   },
   "source": [
    "### Computing RMSE\n",
    "\n",
    "We'll be using the function [``np.polyval``](https://docs.scipy.org/doc/numpy/reference/generated/numpy.polyfit.html) to evaluate our polynomial. Let's see what the ``y_predicted`` or $\\hat y$ value would be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 230,
     "status": "ok",
     "timestamp": 1675261901954,
     "user": {
      "displayName": "Donatella Taurasi",
      "userId": "00673418551725000492"
     },
     "user_tz": -60
    },
    "id": "GcMyC5iZ-pyh",
    "outputId": "1175520e-0fed-4c3a-c5ae-82f1931d3cb8"
   },
   "outputs": [],
   "source": [
    "y_predicted = np.polyval(linear_fit, x)\n",
    "y_predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l1esbFjvFi51"
   },
   "source": [
    "We can compute the residual for each sample by just subtracting one from the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 297,
     "status": "ok",
     "timestamp": 1675261931568,
     "user": {
      "displayName": "Donatella Taurasi",
      "userId": "00673418551725000492"
     },
     "user_tz": -60
    },
    "id": "UBA0qMyXFnG4",
    "outputId": "95c358ab-9051-4b74-c019-87cb208cac9e"
   },
   "outputs": [],
   "source": [
    "linear_fit_residual = y - y_predicted\n",
    "linear_fit_residual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XQ5mHlJRFUo3"
   },
   "source": [
    "Let's compute the RMSE, since that will give us a single value for our fit.\n",
    "\n",
    "We'll compute the \"root\" part of RMSE using ``np.sqrt``, the \"mean\" part of RMSE using ``np.mean``, and the \"square\" part of RMSE using ``np.square`` as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 227,
     "status": "ok",
     "timestamp": 1675261978991,
     "user": {
      "displayName": "Donatella Taurasi",
      "userId": "00673418551725000492"
     },
     "user_tz": -60
    },
    "id": "9LDIS9InFQxA",
    "outputId": "eb7274f4-d080-4906-f185-39ce67aebad8"
   },
   "outputs": [],
   "source": [
    "linear_fit_rmse = np.sqrt(np.mean(np.square(linear_fit_residual)))\n",
    "print(\"Linear fit RMSE = \" + str(linear_fit_rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BcIfCfWbGirO"
   },
   "source": [
    "### Plotting the fit\n",
    "\n",
    "A more intuitive way to look at a fit would be to plot it next to the data that we want to view.\n",
    "\n",
    "To do this, we will need to create data for the $x$ values line. Let's call this ``x_line``, and use [``np.arange``](https://docs.scipy.org/doc/numpy/reference/generated/numpy.arange.html) to set it to be taking 200 samples from ``x.min()`` to ``x.max()``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 711
    },
    "executionInfo": {
     "elapsed": 228,
     "status": "ok",
     "timestamp": 1675262040269,
     "user": {
      "displayName": "Donatella Taurasi",
      "userId": "00673418551725000492"
     },
     "user_tz": -60
    },
    "id": "XmFZdlaIF12W",
    "outputId": "422b7517-6389-4d06-83de-3ef511e33893"
   },
   "outputs": [],
   "source": [
    "x_line_step = (x.max() - x.min()) / 200\n",
    "x_line = np.arange(x.min(), x.max(), x_line_step)\n",
    "display(x_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VbPFOeDsH-67"
   },
   "source": [
    "Now we can plot the our data and the prediction together. We'll use ``np.polyval`` to evaluate ``linear_fit`` at ``x_line``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "executionInfo": {
     "elapsed": 273,
     "status": "ok",
     "timestamp": 1675262043838,
     "user": {
      "displayName": "Donatella Taurasi",
      "userId": "00673418551725000492"
     },
     "user_tz": -60
    },
    "id": "BimG4K5xHSWG",
    "outputId": "d417a696-42f6-45e1-eaf6-6290e5cad681"
   },
   "outputs": [],
   "source": [
    "plt.scatter(x, y, label='Data')\n",
    "plt.plot(x_line, np.polyval(linear_fit, x_line), label='Linear Fit')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6AiXXQ-9QvSe"
   },
   "source": [
    "Do you feel like this fit captures the underlying trend of the data? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rWem_b8DIYGp"
   },
   "source": [
    "## Coding Dojo: Quadratic fit\n",
    "\n",
    "The linear fit doesn't fit our data very well. So maybe we can do better if we try to fit a slightly more complicated model -- a quadratic polynomial. We can do this again with ``np.polyfit``, with the only difference being that we'll use ``deg=2``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x19mS8F1TRm4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K0QBQvSGJwgt"
   },
   "source": [
    "That's great! If going from $x$ to $x^2$ was good, then maybe $x^3$ is better! Maybe $x^4$ is better!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W9ng3sx0LSKG"
   },
   "source": [
    "### Creating a function\n",
    "\n",
    "We'll be running the same code over and over and over again, so it'll be convenient to write a function to do it for us. Let's write the function ``FitPolynomial``, which will have ``P`` be the degree of our polynomial to fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 240,
     "status": "ok",
     "timestamp": 1675262429237,
     "user": {
      "displayName": "Donatella Taurasi",
      "userId": "00673418551725000492"
     },
     "user_tz": -60
    },
    "id": "NSM-JY9vLfHE"
   },
   "outputs": [],
   "source": [
    "def FitPolynomial(P):\n",
    "  # Compute and print the polynomial fit!\n",
    "  degree_P_fit = np.poly1d(np.polyfit(x, y, deg=P))\n",
    "  print('y = ')\n",
    "  print(degree_P_fit)\n",
    "  # Compute and print the RMSE\n",
    "  degree_P_fit_residual = y - np.polyval(degree_P_fit, x)\n",
    "  degree_P_fit_rmse = np.sqrt(np.mean(np.square(degree_P_fit_residual)))\n",
    "  print(str(P) + 'th degree fit RMSE = ' + str(degree_P_fit_rmse))\n",
    "  # Plot the fit\n",
    "  plt.plot(x_line, np.polyval(degree_P_fit, x_line), label=str(P) + 'th Degree Fit')\n",
    "  plt.scatter(x, y, label='Data')\n",
    "  # Use the function plt.ylim to ensure that we see the same y axis for every fit\n",
    "  # (we'll see why shortly)\n",
    "  plt.ylim([0, 1.2])\n",
    "  plt.legend()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RO2Xk-wsL7uU"
   },
   "source": [
    "## Cubic fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 338
    },
    "executionInfo": {
     "elapsed": 271,
     "status": "ok",
     "timestamp": 1675262499173,
     "user": {
      "displayName": "Donatella Taurasi",
      "userId": "00673418551725000492"
     },
     "user_tz": -60
    },
    "id": "b2JvoVuQJtlI",
    "outputId": "253f8498-7d33-40d7-858c-c3f7018700a7"
   },
   "outputs": [],
   "source": [
    "FitPolynomial(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "crSLhXSPLP1M"
   },
   "source": [
    "## 4th degree fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 338
    },
    "executionInfo": {
     "elapsed": 267,
     "status": "ok",
     "timestamp": 1675262535358,
     "user": {
      "displayName": "Donatella Taurasi",
      "userId": "00673418551725000492"
     },
     "user_tz": -60
    },
    "id": "ZlFXtuyNKLl-",
    "outputId": "663dc23b-5196-44e6-a008-a15fa45604c7"
   },
   "outputs": [],
   "source": [
    "FitPolynomial(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pq05AEiXMO1C"
   },
   "source": [
    "## 6th degree fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 338
    },
    "executionInfo": {
     "elapsed": 311,
     "status": "ok",
     "timestamp": 1675262548084,
     "user": {
      "displayName": "Donatella Taurasi",
      "userId": "00673418551725000492"
     },
     "user_tz": -60
    },
    "id": "-Pm-ppdFMEhT",
    "outputId": "893802bb-3dda-400a-ecd4-533c45efbd20"
   },
   "outputs": [],
   "source": [
    "FitPolynomial(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MH0DVFIlMTje"
   },
   "source": [
    "## 9th degree fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "executionInfo": {
     "elapsed": 254,
     "status": "ok",
     "timestamp": 1675262574745,
     "user": {
      "displayName": "Donatella Taurasi",
      "userId": "00673418551725000492"
     },
     "user_tz": -60
    },
    "id": "kHJeJYmxMGEK",
    "outputId": "d687d956-1fce-4a2b-9ca0-e6cddb01b997"
   },
   "outputs": [],
   "source": [
    "FitPolynomial(9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qk70jYjVMhp3"
   },
   "source": [
    "## 12th degree fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "executionInfo": {
     "elapsed": 254,
     "status": "ok",
     "timestamp": 1675262583882,
     "user": {
      "displayName": "Donatella Taurasi",
      "userId": "00673418551725000492"
     },
     "user_tz": -60
    },
    "id": "0T85SdGkMGqp",
    "outputId": "51663acf-acb3-4dd3-87b4-2f6868af4a10"
   },
   "outputs": [],
   "source": [
    "FitPolynomial(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LDCohvBnMqYM"
   },
   "source": [
    "What about this fit? Does it still look good to you?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hxkAGf05Mqe7"
   },
   "source": [
    "## 16th degree fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "executionInfo": {
     "elapsed": 277,
     "status": "ok",
     "timestamp": 1675262589073,
     "user": {
      "displayName": "Donatella Taurasi",
      "userId": "00673418551725000492"
     },
     "user_tz": -60
    },
    "id": "ubAziVL2MnYe",
    "outputId": "68bec9bb-44d7-4c29-f3f0-6100006abe27"
   },
   "outputs": [],
   "source": [
    "FitPolynomial(16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZC0jitNbMppg"
   },
   "source": [
    "## Finding the Best Hyperparameter\n",
    "\n",
    "But our RMSE has kept going down!\n",
    "\n",
    "Let's plot polynomial degree on the X axis versus RMSE on the Y axis all the way through from degree 0 (a constant) through a degree 16 polynomial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "executionInfo": {
     "elapsed": 286,
     "status": "ok",
     "timestamp": 1675262795091,
     "user": {
      "displayName": "Donatella Taurasi",
      "userId": "00673418551725000492"
     },
     "user_tz": -60
    },
    "id": "wRhaItNfPEs5",
    "outputId": "a3416881-cd84-4299-8bd4-11fa983da023"
   },
   "outputs": [],
   "source": [
    "# This is going to be our X data. Let's call it degrees and set it to all of\n",
    "# the degrees we will check, from 0 through 16.\n",
    "degrees = np.arange(17)\n",
    "\n",
    "# This is going to be our Y data. It will be the RMSE for each degree. We don't\n",
    "# know the values yet (we will compute them), so let's initialize it to zero.\n",
    "rmse_for_degree = np.zeros(17)\n",
    "\n",
    "# And for each degree, let's compute the RMSE\n",
    "for P in degrees:\n",
    "  degree_P_fit = np.poly1d(np.polyfit(x, y, deg=P))\n",
    "  residual = y - np.polyval(degree_P_fit, x)\n",
    "  rmse = np.sqrt(np.mean(np.square(residual)))\n",
    "  # Save the RMSE in our Y data\n",
    "  rmse_for_degree[P] = rmse\n",
    "\n",
    "# And let's plot the results.\n",
    "plt.plot(degrees, rmse_for_degree)\n",
    "plt.xlabel('Polynomial degree')\n",
    "plt.ylabel('RMSE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dfpo5tpPRPwf"
   },
   "source": [
    "Our original goal was to find the best hyperparameter (the polynomial degree that is the best fit for our data), but it's not clear from this graph what that is!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SZJjQOIXRm9h"
   },
   "source": [
    "# Training/Test Split\n",
    "\n",
    "In this section, we'll split our data into two sets -- one for testing and one for training. We'll tune our hyperparameter by evaluating how a model trained on the training set performs on the testing set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F4tcY8I1ZCEh"
   },
   "source": [
    "## Splitting the data\n",
    "\n",
    "Here we produce a set of indices for training and testing. The indices are numbers from ``0`` to  ``n`` that indicate positions in the ``x`` and ``y`` arrays.\n",
    "\n",
    "We do this by using the [``sklearn.model_selection.train_test_split``](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function.\n",
    "\n",
    "We'll split the indices evenly using the ``test_size`` parameter. The ``random_state`` parameter sets the random seed for the operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 228,
     "status": "ok",
     "timestamp": 1675262905558,
     "user": {
      "displayName": "Donatella Taurasi",
      "userId": "00673418551725000492"
     },
     "user_tz": -60
    },
    "id": "Xd5gzmklX_-P",
    "outputId": "7fa85832-237b-48f0-fc76-fc9582781410"
   },
   "outputs": [],
   "source": [
    "all_indices = np.arange(n)\n",
    "print('All indices:')\n",
    "print(all_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 303,
     "status": "ok",
     "timestamp": 1675263025038,
     "user": {
      "displayName": "Donatella Taurasi",
      "userId": "00673418551725000492"
     },
     "user_tz": -60
    },
    "id": "7P9--_uaQJB8",
    "outputId": "0efe7d8c-7968-43a6-9312-5c8ae391f5d2"
   },
   "outputs": [],
   "source": [
    "train_indices, test_indices = sklearn.model_selection.train_test_split(\n",
    "    all_indices, test_size=0.5, shuffle=True, random_state=1)\n",
    "train_indices = np.sort(train_indices)\n",
    "test_indices = np.sort(test_indices)\n",
    "print('Training set indices:')\n",
    "print(train_indices)\n",
    "# print(np.sort(train))\n",
    "print('Test set indices:')\n",
    "print(test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 253,
     "status": "ok",
     "timestamp": 1675263057838,
     "user": {
      "displayName": "Donatella Taurasi",
      "userId": "00673418551725000492"
     },
     "user_tz": -60
    },
    "id": "CVRovxxPVUGw",
    "outputId": "285a913b-6d67-4cdd-fae8-0b003454b8b6"
   },
   "outputs": [],
   "source": [
    "x_train = x[train_indices]\n",
    "y_train = y[train_indices]\n",
    "x_test  = x[test_indices]\n",
    "y_test  = y[test_indices]\n",
    "\n",
    "print('x = ')\n",
    "print(x)\n",
    "print('x_train = ')\n",
    "print(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SWsSM-zSZyFm"
   },
   "source": [
    "Let's visualize the split by plotting the training and the testing data in different colors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "executionInfo": {
     "elapsed": 268,
     "status": "ok",
     "timestamp": 1675263099888,
     "user": {
      "displayName": "Donatella Taurasi",
      "userId": "00673418551725000492"
     },
     "user_tz": -60
    },
    "id": "yQ0UbjW2SUD3",
    "outputId": "6b9b73b0-c445-4ac4-ce05-d6c3f171a1fb"
   },
   "outputs": [],
   "source": [
    "plt.scatter(x_train, y_train, label='Train', c='b')\n",
    "plt.scatter(x_test, y_test, label='Test', c='r')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x7GGbGDaZ-o0"
   },
   "source": [
    "## Linear fit\n",
    "\n",
    "Now let's compute a linear (``deg=1``) fit on the training data, and look at its plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "executionInfo": {
     "elapsed": 707,
     "status": "ok",
     "timestamp": 1675263149188,
     "user": {
      "displayName": "Donatella Taurasi",
      "userId": "00673418551725000492"
     },
     "user_tz": -60
    },
    "id": "rgewKKK4Sqwd",
    "outputId": "83552532-b7f3-4d74-83fb-935aee6b2d4d"
   },
   "outputs": [],
   "source": [
    "# Compute our linear fit \n",
    "linear_fit = np.poly1d(np.polyfit(x_train, y_train, deg=1))\n",
    "# Plot the fit\n",
    "plt.scatter(x_train, y_train, label='Train', c='b')\n",
    "plt.scatter(x_test,  y_test,  label='Test', c='r')\n",
    "plt.plot(x_line, np.polyval(linear_fit, x_line), label='Linear Fit', c='b')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "djn4uBS2aWbE"
   },
   "source": [
    "Let's compute the RMSE for this model on the training data and the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 279,
     "status": "ok",
     "timestamp": 1675263182580,
     "user": {
      "displayName": "Donatella Taurasi",
      "userId": "00673418551725000492"
     },
     "user_tz": -60
    },
    "id": "IXguD7oNVaYF",
    "outputId": "7a8d887c-e63d-4f35-a1ca-6edad056a27b"
   },
   "outputs": [],
   "source": [
    "train_residual = y_train - np.polyval(linear_fit, x_train)\n",
    "train_rmse = np.sqrt(np.mean(np.square(train_residual)))\n",
    "print(\"RMSE training data = \" + str(train_rmse))\n",
    "test_residual = y_test - np.polyval(linear_fit, x_test)\n",
    "test_rmse = np.sqrt(np.mean(np.square(test_residual)))\n",
    "print(\"RMSE testing data = \" + str(test_rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rP-4wrLUDlal"
   },
   "source": [
    "## Creating a function\n",
    "\n",
    "As before, we're going to take a look at how this performs for many different polynomial degrees. Let's create a function to do the repetetive work for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 213,
     "status": "ok",
     "timestamp": 1675263274648,
     "user": {
      "displayName": "Donatella Taurasi",
      "userId": "00673418551725000492"
     },
     "user_tz": -60
    },
    "id": "Z45bq7HnD25U"
   },
   "outputs": [],
   "source": [
    "def FitPolynomailToTrainTestSplit(P):\n",
    "  # Compute our polynomial fit\n",
    "  degree_P_fit = np.poly1d(np.polyfit(x_train, y_train, deg=P))\n",
    "  \n",
    "  # Plot the fit\n",
    "  plt.scatter(x_train, y_train, label='Train', c='b')\n",
    "  plt.scatter(x_test, y_test, label='Test', c='r')\n",
    "  plt.plot(x_line, np.polyval(degree_P_fit, x_line),\n",
    "           label='Degree ' + str(P) + ' Fit', c='b')\n",
    "  plt.legend()\n",
    "  plt.show()\n",
    "  \n",
    "  # Compute and print the RMSE\n",
    "  train_residual = y_train - np.polyval(degree_P_fit, x_train)\n",
    "  train_rmse = np.sqrt(np.mean(np.square(train_residual)))\n",
    "  print(\"RMSE training data = \" + str(train_rmse))\n",
    "  test_residual = y_test - np.polyval(degree_P_fit, x_test)\n",
    "  test_rmse = np.sqrt(np.mean(np.square(test_residual)))\n",
    "  print(\"RMSE testing data = \" + str(test_rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ce7szo3val3J"
   },
   "source": [
    "## Quadratic fit\n",
    "\n",
    "Let's run the same code as before, now with ``deg=2``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "executionInfo": {
     "elapsed": 274,
     "status": "ok",
     "timestamp": 1675263284809,
     "user": {
      "displayName": "Donatella Taurasi",
      "userId": "00673418551725000492"
     },
     "user_tz": -60
    },
    "id": "lKVuUNE5X5Wf",
    "outputId": "52f941f1-b59a-4663-97c4-1a67d7288cd0"
   },
   "outputs": [],
   "source": [
    "FitPolynomailToTrainTestSplit(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mNjGP2S1bgKs"
   },
   "source": [
    "Indeed the RMSE is going down like before, for both the training and testing sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W3ulpnKaUCAM"
   },
   "source": [
    "## Coding Dojo: Run the 6th polynomial fit! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1675260758983,
     "user": {
      "displayName": "Donatella Taurasi",
      "userId": "00673418551725000492"
     },
     "user_tz": -60
    },
    "id": "U0bUpAi1UgGE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EKD5xflYbWyS"
   },
   "source": [
    "## 8th degree fit\n",
    "\n",
    "Let's do the same thing, now with a 8th degree polynomial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "executionInfo": {
     "elapsed": 243,
     "status": "ok",
     "timestamp": 1675263336031,
     "user": {
      "displayName": "Donatella Taurasi",
      "userId": "00673418551725000492"
     },
     "user_tz": -60
    },
    "id": "Ht68napdbxbd",
    "outputId": "29c94ef3-63a8-4c33-eaa4-8283b74b7271"
   },
   "outputs": [],
   "source": [
    "FitPolynomailToTrainTestSplit(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ABBI7QuciUr"
   },
   "source": [
    "Something interesting has happened here! Our training data's error is going down, but our testing data has spiked up!\n",
    "\n",
    "And if we look at our plot, the line fits the blue dots well, but is missing the red dots by a lot!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GCAMHwTlcwoC"
   },
   "source": [
    "## Finding the Best Hyperparameter (again!)\n",
    "\n",
    "We want to know the relationship between RMSE on the *test* set versus the polynomial degree.\n",
    "\n",
    "Let's plot that for all polynomial degrees from 0 to 13 (along with the RMSE for the training error)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "executionInfo": {
     "elapsed": 257,
     "status": "ok",
     "timestamp": 1675263481834,
     "user": {
      "displayName": "Donatella Taurasi",
      "userId": "00673418551725000492"
     },
     "user_tz": -60
    },
    "id": "mlUyII6nb1fI",
    "outputId": "1e8ee16b-8bf7-4a42-eab9-9371b6012f8d"
   },
   "outputs": [],
   "source": [
    "# This is going to be our X data. Like last time, it's the degrees we'll test,\n",
    "# this time 0 through 12.\n",
    "degrees = np.arange(13)\n",
    "\n",
    "# This is going to be our Y data. We have two plots now, the RMSE for training\n",
    "# data and RMSE for test data.\n",
    "train_rmse_for_degree = np.zeros(len(degrees))\n",
    "test_rmse_for_degree = np.zeros(len(degrees))\n",
    "\n",
    "# And for each degree, let's compute the RMSE\n",
    "for P in degrees:\n",
    "  # Compute our fit\n",
    "  degree_P_fit = np.poly1d(np.polyfit(x_train, y_train, deg=P))\n",
    "\n",
    "  # Compute our residual and RMSE for training and testing\n",
    "  train_residual = y_train - np.polyval(degree_P_fit, x_train)\n",
    "  train_rmse = np.sqrt(np.mean(np.square(train_residual)))\n",
    "  test_residual = y_test - np.polyval(degree_P_fit, x_test)\n",
    "  test_rmse = np.sqrt(np.mean(np.square(test_residual)))\n",
    "  \n",
    "  # Save the RMSE in our Y axis data\n",
    "  train_rmse_for_degree[P] = train_rmse\n",
    "  test_rmse_for_degree[P] = test_rmse\n",
    "\n",
    "  # And let's plot the results.\n",
    "plt.plot(degrees, train_rmse_for_degree, c='b', label='Train')\n",
    "plt.plot(degrees, test_rmse_for_degree, c='r', label='Test')\n",
    "plt.xlabel('Polynomial degree')\n",
    "plt.ylabel('RMSE')\n",
    "plt.ylim([0, 1])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dVaai6-OdwdA"
   },
   "source": [
    "This gives us a clearer picture of what range of hyperparameters may be best for this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PVn-akRgjznn"
   },
   "source": [
    "# k-Fold Cross-Validation\n",
    "\n",
    "In the above example, we dividied our data at random into 2 sets -- one for training and one for testing. Now we know that the correct way to approach model evalutation is to include a \"validation\" set as well. \n",
    "\n",
    "Let's see how that is done in the context of cross validation.\n",
    "\n",
    "For cross-validation, the training data is divided into folds, each of which is treated as its own validation data set, using all of the other training data as its training data set.\n",
    "\n",
    "We can use the [``sklearn.model_selection.KFold``](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html) function to generate mutliple training and validation sets.\n",
    "\n",
    "In this example, let's do 4-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6fNJYciOsST0"
   },
   "source": [
    "## Visualizing the split\n",
    "\n",
    "Let's print out the 4 different sets of test and training indices that we'll use. For each fold, the training indices are everything not being used for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 612
    },
    "executionInfo": {
     "elapsed": 242,
     "status": "ok",
     "timestamp": 1675263809631,
     "user": {
      "displayName": "Donatella Taurasi",
      "userId": "00673418551725000492"
     },
     "user_tz": -60
    },
    "id": "4A3fqUbaqjzv",
    "outputId": "b82efb44-decb-4181-b3a4-930496a371ff"
   },
   "outputs": [],
   "source": [
    "# kf_xval is what we will use to iterate over our splits\n",
    "n_folds =4\n",
    "kf_xval = sklearn.model_selection.KFold(n_splits=n_folds, shuffle=True, random_state=7)\n",
    "\n",
    "# We'll want to label each fold as 0, 1, 2, 3 on the graph and when we print\n",
    "# the data. We'll use the fold_index variable for that.\n",
    "fold_number = 0\n",
    "for fold_train_indices, fold_valid_indices in kf_xval.split(x_train):\n",
    "  # fold_train_indices, fold_valid_indices are the indices from 0 through 15\n",
    "  # (the size of x_train). Print them.\n",
    "  print('Fold ' + str(fold_number) + ' training indices: ')\n",
    "  print(fold_train_indices)\n",
    "  print('Fold ' + str(fold_number) + ' validation indices: ')\n",
    "  print(fold_valid_indices)\n",
    "  print('')\n",
    "\n",
    "  # Use them to sub-divide x_train and y_train into the training and validation\n",
    "  # sets for this fold.\n",
    "  x_fold_train = x_train[fold_train_indices]\n",
    "  y_fold_train = y_train[fold_train_indices]\n",
    "  x_fold_valid = x_train[fold_valid_indices]\n",
    "  y_fold_valid = y_train[fold_valid_indices]\n",
    "\n",
    "  # Plot the fold's valiation data points\n",
    "  plt.scatter(x_fold_valid, y_fold_valid, label='Fold ' + str(fold_number))\n",
    "  \n",
    "  # Increment our fold index.\n",
    "  fold_number += 1\n",
    "\n",
    "# Show the plot\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TegLJl9bsjWI"
   },
   "source": [
    "## Finding the best Hyperparameter (again again!)\n",
    "\n",
    "In the code below, we try all of our different degree fits on the above sets, and we plot the resulting error for each.\n",
    "\n",
    "Our X axis is polynomial degree. On the Y axis we'll have the mean training fit and the mean validation fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "executionInfo": {
     "elapsed": 284,
     "status": "ok",
     "timestamp": 1675263976634,
     "user": {
      "displayName": "Donatella Taurasi",
      "userId": "00673418551725000492"
     },
     "user_tz": -60
    },
    "id": "Vx5u0KnWdaD3",
    "outputId": "da67b95f-fd18-4384-9171-d8912ef63bd2"
   },
   "outputs": [],
   "source": [
    "# This is going to be our X data. Like last time, it's the degrees we'll test.\n",
    "degrees = np.arange(11)\n",
    "\n",
    "# This is going to be our Y data. We have two plots now, the RMSE for training\n",
    "# data and RMSE for test data. Because we need to take the mean of the RMSE for\n",
    "# all folds, we'll create a matrix with entries for each degree and each fold.\n",
    "train_rmse_for_degree = np.zeros([len(degrees), n_folds])\n",
    "valid_rmse_for_degree = np.zeros([len(degrees), n_folds])\n",
    "\n",
    "# We'll use fold_number to keep track of which fold we're on, so we know which\n",
    "# entry to save it in.\n",
    "fold_number = 0\n",
    "\n",
    "# Iterate over every training/validation split\n",
    "for fold_train_indices, fold_valid_indices in kf_xval.split(x_train):\n",
    "  # fold_train_indices and fold_valid_indices are the indices. Use them to\n",
    "  # sub-divide x_train and y_train into the training and validation sets for\n",
    "  # this fold.\n",
    "  x_fold_train = x_train[fold_train_indices]\n",
    "  y_fold_train = y_train[fold_train_indices]\n",
    "  x_fold_valid = x_train[fold_valid_indices]\n",
    "  y_fold_valid = y_train[fold_valid_indices]\n",
    "\n",
    "  # Iterate over every polynomial degree P (this is now exactly like before)\n",
    "  for P in degrees:\n",
    "    # Fit the degree P polynomial using the fold's training data\n",
    "    degree_P_fit = np.poly1d(np.polyfit(x_fold_train, y_fold_train, deg=P))\n",
    "\n",
    "    # Compute the residual and RMSE for training and validation\n",
    "    train_residual = y_fold_train - np.polyval(degree_P_fit, x_fold_train)\n",
    "    valid_residual = y_fold_valid - np.polyval(degree_P_fit, x_fold_valid)\n",
    "    train_rmse = np.sqrt(np.mean(np.square(train_residual)))\n",
    "    valid_rmse = np.sqrt(np.mean(np.square(valid_residual)))\n",
    "    \n",
    "    # Save the RMSE\n",
    "    train_rmse_for_degree[P, fold_number] = train_rmse\n",
    "    valid_rmse_for_degree[P, fold_number] = valid_rmse\n",
    "\n",
    "  # Increment the index for the fold we're on.\n",
    "  fold_number += 1\n",
    "\n",
    "# Now let's get the mean training and validation RMSE across all folds\n",
    "mean_train_rmse_for_degree = np.mean(train_rmse_for_degree,1)\n",
    "mean_valid_rmse_for_degree = np.mean(valid_rmse_for_degree,1)\n",
    "\n",
    "# print('Training RMSE matrix. Folds are horizontal, degree is vertical')\n",
    "# print(train_rmse_for_degree)\n",
    "# print('Mean training RMSE for each degree.')\n",
    "# print(mean_train_rmse_for_degree)\n",
    "  \n",
    "# And let's plot them!\n",
    "plt.plot(degrees, mean_train_rmse_for_degree, label='Mean Training RMSE')\n",
    "plt.plot(degrees, mean_valid_rmse_for_degree, label='Mean Validation RMSE')\n",
    "plt.xlabel('Polynomial degree')\n",
    "plt.ylabel('RMSE')\n",
    "plt.ylim([0, 0.5])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NcZeDBFpQeo7"
   },
   "source": [
    "## Evaluating the model on the test data\n",
    "\n",
    "From the above plot, a degree $P=2$ fit is the best hyperparameter selection. Let's now train our model on all training data using that hyperparameter, and evaluate it on the test set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 244,
     "status": "ok",
     "timestamp": 1675264126380,
     "user": {
      "displayName": "Donatella Taurasi",
      "userId": "00673418551725000492"
     },
     "user_tz": -60
    },
    "id": "2ARLN-peQx_2",
    "outputId": "524f807c-8722-48f4-9ec5-c66c3e619ab5"
   },
   "outputs": [],
   "source": [
    "best_P = 2\n",
    "best_P_fit = np.poly1d(np.polyfit(x_train, y_train, deg=best_P))\n",
    "\n",
    "best_P_test_residual = y_test - np.polyval(best_P_fit, x_test)\n",
    "best_P_test_rmse = np.sqrt(np.mean(np.square(best_P_test_residual)))\n",
    "\n",
    "print('P=2 Test RMSE = ' + str(best_P_test_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "executionInfo": {
     "elapsed": 300,
     "status": "ok",
     "timestamp": 1675264131799,
     "user": {
      "displayName": "Donatella Taurasi",
      "userId": "00673418551725000492"
     },
     "user_tz": -60
    },
    "id": "aDZCrLJdROdh",
    "outputId": "5a82fd7f-acaa-498b-87b1-ce5b777f9435"
   },
   "outputs": [],
   "source": [
    "# And let's everything together!\n",
    "plt.plot(degrees, mean_train_rmse_for_degree, label='Mean Training RMSE')\n",
    "plt.plot(degrees, mean_valid_rmse_for_degree, label='Mean Validation RMSE')\n",
    "plt.scatter([best_P], [best_P_test_rmse], label='Test RMSE', c='r')\n",
    "plt.xlabel('Polynomial degree')\n",
    "plt.ylabel('RMSE')\n",
    "plt.ylim([0, 0.5])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hFqV286s_DOa"
   },
   "source": [
    "# Coding Dojo: Leave-one-out cross validation\n",
    "\n",
    "Try modifying the $k-$fold cross validation example above to use a [``LeaveOneOut``](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.LeaveOneOut.html) cross-validator instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tTj6395x_GKH"
   },
   "outputs": [],
   "source": [
    "# We used kf_xval before. Let's use xf_loocv\n",
    "loo_xval = sklearn.model_selection.LeaveOneOut()\n",
    "\n",
    "# the number of folds we'll have now is the length of the training data.\n",
    "n_folds = len(x_train)\n",
    "\n",
    "# Copy-paste what we did for k-fold cross validation here, now using loo_xval\n",
    "# instead of kf_xval. How does it compare?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fi_ma38WVjIb"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1zlvTnF92gXVv8Yonu_hcJm_85PYSSYtA",
     "timestamp": 1554749785373
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
